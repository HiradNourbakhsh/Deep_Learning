# -*- coding: utf-8 -*-
"""BatchNorm_Dropout

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z-trmHjbhssSeKlu4moVjgeZAzvAAA2Z

# BN and DO
Task:

Train 4 neural networks with the functional API with the same number of neurons (at least 3 layers with at least 100 neurons each) for the digits dataset:
1. One model should have neither dropout or batch norm
2. One model should have batch norm
3. One model should have dropout
4. One model should have both Dropout and Batch Norm

Plot the results of the accuracy of the four different models on the same graph
"""

from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

digits = load_digits()
X, y = digits.data, digits.target

_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))
for ax, image, label in zip(axes, digits.images, digits.target):
    ax.set_axis_off()
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    ax.set_title('Training: %i' % label)

X.shape

y.shape

X.dtype

y.dtype



X.shape

y.shape

X.shape[1]

import tensorflow as tf
from tensorflow import keras

"""# Functional API - Neither BN nor DO"""

input_ = keras.layers.Input(shape = X.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(100, activation="relu")(flatten)
hidden2 = keras.layers.Dense(100, activation="relu")(hidden1)
hidden3 = keras.layers.Dense(100, activation = 'relu')(hidden2)
concat = keras.layers.concatenate([flatten, hidden3])
output = keras.layers.Dense(10, activation = 'softmax')(concat)

model_1 = keras.models.Model(inputs=[input_], outputs=[output])

model_1.summary()

model_1.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
                  metrics=["accuracy"])

history_1 = model_1.fit(X, y, epochs=20)

import pandas as pd

pd.DataFrame(history_1.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

neither = pd.DataFrame(history_1.history)

neither

"""# Batch Normalization"""

input_ = keras.layers.Input(shape = X.shape[1:])
flatten = keras.layers.Flatten()(input_)
bn1 = keras.layers.BatchNormalization()(flatten)
hidden1 = keras.layers.Dense(100, activation="relu")(bn1)
bn2 = keras.layers.BatchNormalization()(hidden1)
hidden2 = keras.layers.Dense(100, activation="relu")(bn2)
bn3 = keras.layers.BatchNormalization()(hidden2)
hidden3 = keras.layers.Dense(100, activation = 'relu')(bn3)
bn4 = keras.layers.BatchNormalization()(hidden3)
concat = keras.layers.concatenate([flatten, bn4])
output = keras.layers.Dense(10, activation = 'softmax')(concat)

model_2 = keras.models.Model(inputs=[input_], outputs=[output])

model_2.summary()

model_2.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
                  metrics=["accuracy"])

history_2 = model_2.fit(X, y, epochs=20)

import pandas as pd

pd.DataFrame(history_2.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

batchnorm = pd.DataFrame(history_2.history)

batchnorm

"""# Dropout"""

input_ = keras.layers.Input(shape = X.shape[1:])
flatten = keras.layers.Flatten()(input_)
dp1 = keras.layers.Dropout(rate = 0.2)(flatten)
hidden1 = keras.layers.Dense(100, activation="relu")(dp1)
dp2 = keras.layers.Dropout(rate = 0.2)(hidden1)
hidden2 = keras.layers.Dense(100, activation="relu")(dp2)
dp3 = keras.layers.Dropout(rate = 0.2)(hidden2)
hidden3 = keras.layers.Dense(100, activation = 'relu')(dp3)
concat = keras.layers.concatenate([flatten, hidden3])
output = keras.layers.Dense(10, activation = 'softmax')(concat)

model_3 = keras.models.Model(inputs=[input_], outputs=[output])

model_3.summary()

model_3.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
                  metrics=["accuracy"])

history_3 = model_3.fit(X, y, epochs = 20)

import pandas as pd

pd.DataFrame(history_3.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

drop = pd.DataFrame(history_3.history)

drop

"""# BN and DO"""

input_ = keras.layers.Input(shape = X.shape[1:])
flatten = keras.layers.Flatten()(input_)
dp1 = keras.layers.Dropout(rate = 0.2)(flatten)
hidden1 = keras.layers.Dense(100, activation="relu")(dp1)
bn1 = keras.layers.BatchNormalization()(hidden1)
dp2 = keras.layers.Dropout(rate = 0.2)(bn1)
hidden2 = keras.layers.Dense(100, activation="relu")(dp2)
bn2 = keras.layers.BatchNormalization()(hidden2)
dp3 = keras.layers.Dropout(rate = 0.2)(bn2)
hidden3 = keras.layers.Dense(100, activation = 'relu')(dp3)
bn3 = keras.layers.BatchNormalization()(hidden3)
concat = keras.layers.concatenate([flatten, bn3])
output = keras.layers.Dense(10, activation = 'softmax')(concat)

model_4 = keras.models.Model(inputs=[input_], outputs=[output])

model_4.summary()

model_4.compile(loss="sparse_categorical_crossentropy", optimizer="sgd",
                  metrics=["accuracy"])

history_4 = model_4.fit(X, y, epochs = 20)

import pandas as pd

pd.DataFrame(history_4.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

both = pd.DataFrame(history_4.history)

both

"""# Plot"""

neither = neither.rename(columns = {'accuracy': 'neither'})
neither

batchnorm = batchnorm.rename(columns = {'accuracy': 'batch_normalization'})
batchnorm

drop = drop.rename(columns = {'accuracy': 'dropout'})
drop

both = both.rename(columns = {'accuracy': 'both'})
both

concatenated = pd.concat([neither.neither, batchnorm.batch_normalization, drop.dropout, both.both], axis = 1)
concatenated

concatenated.plot(figsize = (15,15))
plt.grid(True)
plt.gca().set_ylim(0.5,1)
plt.show()

