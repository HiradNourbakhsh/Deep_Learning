# -*- coding: utf-8 -*-
"""Hyperparameter_Exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fSZs74WzIJJXIB5qoz37OWovkqa5npyZ

### Task:
Using the functional API:
1. Choose 4 hyperparameters
2. Create lists of the different options of hyperparameters
3. Randomly sample from those lists and train a Keras model on Fashion - MNIST and train 10 different models
4. Display the different loss and accuracy curves using matplotlib
5. Print hyperparameters of the best model
** Train at least 10 different models
"""

import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import pandas as pd
import random

tf.__version__

fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255. # convert inputs into float, standardizes them
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
X_test = X_test / 255.

y_train

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

class_names[y_train[0]]

X_train.shape[1:]

leaky_relu = keras.layers.LeakyReLU(alpha=0.2)
pre_lu = keras.layers.PReLU()

kernel_initializer = ['he_normal', 'lecun_normal', 'glorot_uniform']
activation_function = ['selu', 'relu', 'sigmoid']
optimizer = [keras.optimizers.SGD(lr = 0.001, momentum = 0.9),#momentum 
             keras.optimizers.SGD(lr = 0.001, momentum = 0.9, nesterov = True), #Nesterov accelerated gradient
             keras.optimizers.Adagrad(lr = 0.001), # Adagrad
             keras.optimizers.RMSprop(lr = 0.001, rho = 0.9), #RMSProp
             keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999), # Adam
             keras.optimizers.Adamax(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999), #Adamax
             keras.optimizers.Nadam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999), # Nadam
             keras.optimizers.SGD()]

regularizer = [keras.regularizers.l1(0.01), keras.regularizers.l2(0.01)]

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()



ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

ke = random.choice(kernel_initializer)
af = random.choice(activation_function)
op = random.choice(optimizer)
re = random.choice(regularizer)

print('Initializer: ', ke)
print('Activation function: ', af)
print('Optimizer: ', op)
print('Regularizer', re)

input_ = keras.layers.Input(shape = X_train.shape[1:])
flatten = keras.layers.Flatten()(input_)
hidden1 = keras.layers.Dense(300, activation = af, kernel_initializer = ke, kernel_regularizer = re)(flatten)
hidden2 = keras.layers.Dense(100, activation = af, kernel_initializer = ke, kernel_regularizer = re)(hidden1)
concat = keras.layers.concatenate([flatten, hidden2])
output = keras.layers.Dense(10, activation = 'softmax')(concat) # softmax activation because classes are exclusive
# 10 output neurons corresponding to 10 image classes

model = keras.models.Model(inputs = [input_], outputs = [output])

model.compile(loss = 'sparse_categorical_crossentropy', 
              optimizer = op, 
              metrics = ['accuracy'])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))

import matplotlib.pyplot as plt
import pandas as pd
pd.DataFrame(history.history).plot(figsize = (10, 10))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()

"""Best model validation accuracy: 86.38%
Hyperparameters of best model:
Initializer:  glorot_uniform
Activation function:  relu
Optimizer: Nadam
Regularizer: L2
"""

